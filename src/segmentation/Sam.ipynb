{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-09T11:29:16.237734Z",
     "start_time": "2024-12-09T11:29:08.871470Z"
    }
   },
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from ultralytics import YOLO"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/romainmorin/Desktop/TN/3A/PI/Projet/src/TrainPI/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:29:18.506672Z",
     "start_time": "2024-12-09T11:29:18.116419Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%matplotlib qt\n",
    "#%wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
   ],
   "id": "f57a3ef358a5bcd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:29:19.366002Z",
     "start_time": "2024-12-09T11:29:19.363822Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.getcwd())",
   "id": "728e80e727b6e0fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/romainmorin/Desktop/TN/3A/PI/Projet/src/TrainPI/src/segmentation\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:29:20.905677Z",
     "start_time": "2024-12-09T11:29:20.179589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Chemin vers les poids du modèle\n",
    "sam_checkpoint = \"sam_weights/sam_vit_b_01ec64.pth\"\n",
    "model_type = \"vit_b\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)"
   ],
   "id": "45014e9ee9fb56b4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:29:30.747359Z",
     "start_time": "2024-12-09T11:29:22.316744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "image_path = \"../../data/content/helicopter.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "model = YOLO(\"yolov8x.pt\")\n",
    "results = model(image, conf=0.25)"
   ],
   "id": "aae640206aaf85e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8x.pt to 'yolov8x.pt'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131M/131M [00:06<00:00, 22.4MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 512x640 1 airplane, 590.3ms\n",
      "Speed: 2.1ms preprocess, 590.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:30:06.930280Z",
     "start_time": "2024-12-09T11:29:58.735927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "helicopter_box = None\n",
    "for r in results:\n",
    "    for box in r.boxes:\n",
    "        cls_id = int(box.cls)\n",
    "        cls_name = r.names[cls_id]\n",
    "        if cls_name == \"airplane\":\n",
    "            # Récupérer la bbox (x_min, y_min, x_max, y_max)\n",
    "            x_min, y_min, x_max, y_max = box.xyxy[0].cpu().numpy().astype(int)\n",
    "            helicopter_box = [x_min, y_min, x_max, y_max]\n",
    "            break\n",
    "    if helicopter_box is not None:\n",
    "        break\n",
    "\n",
    "if helicopter_box is None:\n",
    "    print(\"Aucun hélicoptère détecté !\")\n",
    "else:\n",
    "    # Utiliser SAM avec la boîte englobante\n",
    "    predictor = SamPredictor(sam)\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    box_coords = np.array(helicopter_box)\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        box=box_coords[None, :],  # (1,4)\n",
    "        multimask_output=True\n",
    "    )\n"
   ],
   "id": "bf42b6693612fb21",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:30:09.656421Z",
     "start_time": "2024-12-09T11:30:09.653068Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "5cb1499d4df1a730",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "N_POINTS_DESIRES = 1\n",
    "\n",
    "input_points = []\n",
    "input_labels = []\n",
    "\n",
    "# Fonction de rappel pour événement de clic\n",
    "def onclick(event):\n",
    "    ix, iy = event.xdata, event.ydata\n",
    "    if ix is not None and iy is not None:\n",
    "        print(f'Point sélectionné : ({ix}, {iy})')\n",
    "        input_points.append([ix, iy])\n",
    "        input_labels.append(1)  # Indicateur positif\n",
    "\n",
    "        # Marquer le point sur l'image\n",
    "        ax.plot(ix, iy, 'ro')  # point rouge\n",
    "        fig.canvas.draw()\n",
    "\n",
    "        if len(input_points) == N_POINTS_DESIRES:\n",
    "            fig.canvas.mpl_disconnect(cid)\n",
    "            print(\"Nombre de points requis atteints.\")\n",
    "\n",
    "# Affichage de l'image et sélection du point\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "ax.imshow(image)\n",
    "ax.set_title(\"Cliquez sur l'hélicoptère pour sélectionner un point\")\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "plt.show()\n",
    "\n",
    "# Attente jusqu'à ce qu'un point soit sélectionné\n",
    "while len(input_points) < N_POINTS_DESIRES:\n",
    "    plt.pause(0.1)\n",
    "\n",
    "input_point = np.array(input_points)\n",
    "input_label = np.array(input_labels)\n"
   ],
   "id": "544104fb1c98b87a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\"\"\"input_points = np.array([500, 300])\n",
    "input_labels = np.array([1, 0])\"\"\"\n",
    "\n",
    "masks, scores, logits = predictor.predict(\n",
    "    point_coords=input_point,\n",
    "    point_labels=input_label,\n",
    "    multimask_output=True\n",
    ")"
   ],
   "id": "ef8129c10e483774"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:30:23.308235Z",
     "start_time": "2024-12-09T11:30:14.025879Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Afficher les masques\n",
    "fig, axes = plt.subplots(1, len(masks), figsize=(15, 5))\n",
    "for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "    axes[i].imshow(image)\n",
    "    axes[i].imshow(mask, alpha=0.5)\n",
    "    axes[i].set_title(f\"Masque {i+1} - Score : {score:.3f}\")\n",
    "    axes[i].axis('off')\n",
    "plt.show()"
   ],
   "id": "80e59e8207ed74f8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:30:37.640322Z",
     "start_time": "2024-12-09T11:30:37.633786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selected_mask = masks[0]\n",
    "\n",
    "# Post-traitement du masque\n",
    "# 1. Sélection de la plus grande composante connectée\n",
    "mask_uint8 = selected_mask.astype(np.uint8)\n",
    "num_labels, labels_im = cv2.connectedComponents(mask_uint8)\n",
    "\n",
    "largest_label = 0\n",
    "largest_area = 0\n",
    "for label in range(1, num_labels):  # Ignorer l'arrière-plan (label=0)\n",
    "    area = np.sum(labels_im == label)\n",
    "    if area > largest_area:\n",
    "        largest_area = area\n",
    "        largest_label = label\n",
    "\n",
    "final_mask = (labels_im == largest_label).astype(np.uint8)"
   ],
   "id": "4b5cab6fc46db705",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T11:30:38.566737Z",
     "start_time": "2024-12-09T11:30:38.501679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "kernel = np.ones((3, 3), np.uint8)\n",
    "# Ouverture pour supprimer les petits artefacts isolés\n",
    "final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_OPEN, kernel)\n",
    "# Fermeture pour combler les petits trous à l'intérieur du masque\n",
    "final_mask = cv2.morphologyEx(final_mask, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "\n",
    "# Application du masque sur l'image\n",
    "masked_image = image.copy()\n",
    "masked_image[final_mask == 0] = 0  # Mettre le fond à noir\n",
    "cv2.imwrite(\"helicopter_masked.png\", masked_image)\n",
    "# Affichage de l'image masquée finale\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(masked_image)\n",
    "plt.title(\"Hélicoptère isolé avec fond noir\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ],
   "id": "f6916c688463b003",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6acb243a5c0270e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "ac712eb2ab6181c3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
